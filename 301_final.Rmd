---
title: 'DS301: Final Report'
author: "Team GAME: MyTien Kien, Emily Pollock, Aaron Hanrahan, Gajin Kim"
date: "5/8/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r setup2, include=FALSE}
library(ggplot2)
library(ISLR2)
library(dplyr)
library(caret)
library(MASS)
library(class)

#loading in red and white wine datasets
red <- read.csv("./winequality-red.csv", header=TRUE)
white <- read.csv("./winequality-white.csv", header=TRUE)

#for question 2
red$color = "red"
white$color = "white"
wines <- rbind(red, white)
```

### Initial Exploratory Analysis
```{r}
#key variables between red and white wines
ggplot(wines, aes(alcohol, fill = color)) + geom_density(alpha = 0.2)
ggplot(wines, aes(pH, fill = color)) + geom_density(alpha = 0.2)
ggplot(wines, aes(fixed.acidity, fill = color)) + geom_density(alpha = 0.2)
ggplot(wines, aes(volatile.acidity, fill = color)) + geom_density(alpha = 0.2)
ggplot(wines, aes(citric.acid, fill = color)) + geom_density(alpha = 0.2)
ggplot(wines, aes(residual.sugar, fill = color)) + geom_density(alpha = 0.2)
ggplot(wines, aes(chlorides, fill = color)) + geom_density(alpha = 0.2)
ggplot(wines, aes(free.sulfur.dioxide, fill = color)) + geom_density(alpha = 0.2)
ggplot(wines, aes(total.sulfur.dioxide, fill = color)) + geom_density(alpha = 0.2)
ggplot(wines, aes(density, fill = color)) + geom_density(alpha = 0.2)
ggplot(wines, aes(sulphates, fill = color)) + geom_density(alpha = 0.2)
```


### Question 1: *Can we predict red and white wine quality based on their characteristics?*
#### Method 1 - KNN
##### KNN Red Whine
```{r}
red <- read.csv("./winequality-red.csv", header=TRUE)
white <- read.csv("./winequality-white.csv", header=TRUE)
set.seed(9)

red$newquality <- with(red, ifelse(quality > 6, 'great',
                            ifelse(quality > 4, 'good', 'bad')))

red <- subset(red, select = -c(quality))
numruns <- 200
folds <- rep(0, 5)
K = c(1,3,5,7,9) 

test = sample(1:dim(red)[1], 200, replace=FALSE)

standardized.X = scale(red[,-dim(red)[2]])

train.X = standardized.X[-test,]
test.X = standardized.X[test,]
train.Y = red$newquality[-test]
test.Y = red$newquality[test]

red2 = red[-test,]
standardized.X2 = scale(red2[,-dim(red2)[2]])

for (i in 1:numruns) {
  flds <- createFolds(red2$newquality, k = 5, list = TRUE, returnTrain = FALSE)
  
  cv_error = matrix(NA, 5, length(K))
  
  for(j in 1:length(K)) {
    k = K[j]
    for(i in 1:5){
      test_index = flds[[i]]
      testX = standardized.X2[test_index,]
      trainX = standardized.X2[-test_index,]
      
      trainY = red2$newquality[-test_index]
      testY = red2$newquality[test_index]
      
      knn.pred = knn(trainX,testX,trainY,k=k)
      cv_error[i,j] = mean(testY!=knn.pred)
    }
  }
  
  best <- which.min(apply(cv_error, 2, mean))
  folds[best] = folds[best] + 1
}

folds
knn.pred = knn(train.X, test.X, train.Y,k=K[which.max(folds)])
table(knn.pred,test.Y)
1 - mean(test.Y!=knn.pred)
```

##### KNN White Wine
```{r}
set.seed(9)
white <- white[!(white$quality == 9),]

white$newquality <- with(white, ifelse(quality > 6, 'great',
                            ifelse(quality > 4, 'good', 'bad')))

white <- subset(white, select = -c(quality))
numruns <- 50
folds <- rep(0, 5)

test = sample(1:dim(white)[1], 600, replace=FALSE)

standardized.X = scale(white[,-dim(white)[2]])

train.X = standardized.X[-test,]
test.X = standardized.X[test,]
train.Y = white$newquality[-test]
test.Y = white$newquality[test]

white2 = white[-test,]
standardized.X2 = scale(white2[,-dim(white2)[2]])
  
for (i in 1:numruns) {
  flds <- createFolds(white2$newquality, k = 5, list = TRUE, returnTrain = FALSE)
  
  cv_error = matrix(NA, 5, length(K))
  
  for(j in 1:length(K)) {
    k = K[j]
    for(i in 1:5){
      test_index = flds[[i]]
      testX = standardized.X2[test_index,]
      trainX = standardized.X2[-test_index,]
      
      trainY = white2$newquality[-test_index]
      testY = white2$newquality[test_index]
      
      knn.pred = knn(trainX,testX,trainY,k=k)
      cv_error[i,j] = mean(testY!=knn.pred)
    }
  }
  
  best <- which.min(apply(cv_error, 2, mean))
  folds[best] = folds[best] + 1
}

folds
knn.pred = knn(train.X,test.X,train.Y,k=K[which.max(folds)])
table(knn.pred,test.Y)
1 - mean(test.Y!=knn.pred)
```
\

#### Method 2 - QDA
```{r}
red <- read.csv("winequality-red.csv")
white <- read.csv("winequality-white.csv")
white <- white[!(white$quality == 9),] # remove occurrences where quality is 9
red$newquality <- with(red, ifelse(quality > 6, 'great',
                                   ifelse(quality > 4, 'good', 'bad')))
white$newquality <- with(white, ifelse(quality > 6, 'great',
                                   ifelse(quality > 4, 'good', 'bad')))
red <- subset(red, select = -c(quality))
white <- subset(white, select = -c(quality))

train = sample(1:dim(red)[1], 1000, replace=FALSE)
qda.fit = qda(newquality~.,data=red, subset=train)
qda.pred = predict(qda.fit,red[-train,])
table(qda.pred$class,red[-train,]$newquality)
mean(qda.pred$class==red[-train,]$newquality)
# WHITE QDA
train = sample(1:dim(white)[1], 1000, replace=FALSE)
qda.fit = qda(newquality~.,data=white, subset=train)
qda.pred = predict(qda.fit,white[-train,])
table(qda.pred$class,white[-train,]$newquality)
mean(qda.pred$class==white[-train,]$newquality)
```
\


### Question 2: *Can we try to predict wine color based on the characteristics?*
#### Method 1 - Logistic Regression
##### Logistic regression with all predictors
```{r, echo=FALSE}
red <- read.csv("./winequality-red.csv", header=TRUE)
white <- read.csv("./winequality-white.csv", header=TRUE)
red$color = "red"
white$color = "white"

set.seed(1)
train = sample(1:nrow(wines),nrow(wines)/2, replace=FALSE)
test = (-train)
wines$color <- as.factor(wines$color)

wines.fit1 = glm(color~.,data=wines,subset=train,family='binomial')
#train logistic model on training set with all predictors
```

```{r, echo=FALSE}
summary(wines.fit1)
head(wines.fit1$fitted.values)
head(wines[train,])
```

volatile.acidity, residual.sugar, chlorides, free.sulfur.dioxide, total.sulfur.dioxide, density, sulphates and alcohol are all significant at alpha = 0.05


```{r, echo=FALSE}
wines.prob1 = predict(wines.fit1,wines[test,],type='response') 
head(wines.prob1)
head(wines[test,])
```

```{r, echo=FALSE}
wines.pred1 = rep('red',length(test))
wines.pred1[wines.prob1 >0.5] ='white'
table(wines.pred1,wines[test,]$color) 
```

```{r, echo=FALSE}
# what is our misclassification rate? 
1-mean(wines.pred1 == wines[test,]$color)
```

Misclassification rate is 0.7%


##### Logistic regression with select predictors
```{r, echo=FALSE}
wines.fit2 = glm(color~chlorides+volatile.acidity+total.sulfur.dioxide+density,data=wines,subset=train,family='binomial')
#train logistic model on training set with selected predictors
```

```{r, echo=FALSE}
summary(wines.fit2)
head(wines.fit2$fitted.values)
head(wines[train,])
```

All of the predictors are statistically significant at alpha = 0.05

```{r, echo=FALSE}
wines.prob2 = predict(wines.fit2,wines[test,],type='response') 
head(wines.prob2)
head(wines[test,])
```

The predicted probabilities are close to zero, indicating that the wine is red. The actual observations show that the wine color is indeed red for the first six observations.


```{r, echo=FALSE}
wines.pred2 = rep('red',length(test))
wines.pred2[wines.prob2 >0.5] ='white'
table(wines.pred2,wines[test,]$color) 
```

```{r, echo=FALSE}
# what is our misclassification rate? 
1-mean(wines.pred2 == wines[test,]$color)
```

Misclassification error: 2.1%

```{r, echo=FALSE}
# what is our correct classification rate? 
mean(wines.pred2 == wines[test,]$color)
```

```{r, echo=FALSE}
# misclassification rate for red wine
35/(35+764)
```

```{r, echo=FALSE}
# misclassification rate for white wine
34/(34+2416)
```

The misclassification rate is very slightly higher for red wine than white. This could be due to the fact that white wine observations more than triple the red wine observations in the test set. If the difference was significant enough, we could adjust the threshold to improve the classification of red wine.


```{r, echo=FALSE}
table(wines[test,]$color)
```
\

#### Method 2 - LDA
```{r method2setup, warning=FALSE}
red <- read.csv("./winequality-red.csv", header=TRUE)
white <- read.csv("./winequality-white.csv", header=TRUE)
red$color = "red"
white$color = "white"
#transforming the predictor variables for red
#NOT including total.sulfur.dioxide, residual.sugar, and alcohol
q2red <- red[,c(1, 2, 3, 5, 6, 8, 9, 10, 12, 13)]
q2red$fixed.acidity <- log10(q2red$fixed.acidity)
q2red$volatile.acidity <- log10(q2red$volatile.acidity)
q2red$citric.acid <- sqrt(q2red$citric.acid)
q2red$chlorides <- log10(q2red$chlorides)
q2red$pH <- log10(q2red$pH)
q2red$sulphates <- log10(q2red$sulphates)
q2red$free.sulfur.dioxide <- sqrt(q2red$free.sulfur.dioxide)
q2red$density <- sqrt(q2red$density)

#transforming the predictor variables for white
#NOT including total.sulfur.dioxide, residual.sugar, and alcohol
q2white <- white[,c(1, 2, 3, 5, 6, 8, 9, 10, 12, 13)]
q2white$fixed.acidity <- log10(q2white$fixed.acidity)
q2white$volatile.acidity <- log10(q2white$volatile.acidity)
q2white$citric.acid <- sqrt(q2white$citric.acid)
q2white$chlorides <- log10(q2white$chlorides)
q2white$pH <- log10(q2white$pH)
q2white$sulphates <- log10(q2white$sulphates)
q2white$free.sulfur.dioxide <- sqrt(q2white$free.sulfur.dioxide)
q2white$density <- sqrt(q2white$density)

#joining the 2 datasets
q2m2_wines <- rbind(q2red, q2white)
q2m2_wines$color = as.factor(q2m2_wines$color)

#red wine covariance matrix
cov(q2red[,-10])

#white wine covariance matrix
cov(q2white[,-10])
```


```{r}
#this is the lda analysis against all valid predictors
set.seed(1)
train = sample(1:dim(q2m2_wines)[1], dim(q2m2_wines)[1]/2, replace=FALSE)
test = q2m2_wines[-train,]

lda.fit = lda(color~., data=q2m2_wines, subset=train)
lda.pred = predict(lda.fit, test)

lda.fit
head(lda.pred$class)

#confusion matrix
table(lda.pred$class, test$color)

#misclassification rate of 1.7% 
mean(lda.pred$class != test$color)
```
\

```{r}
#performing lda on predictors that differs white wine between red
set.seed(1)
lda.fit = lda(color~chlorides+volatile.acidity+free.sulfur.dioxide+density, data=q2m2_wines, subset=train)
lda.pred = predict(lda.fit, test)

lda.fit
head(lda.pred$class)

#confusion matrix
table(lda.pred$class, test$color)

#misclassification rate of 2.4% 
mean(lda.pred$class != test$color)
```
\




